# Modul Pembelajaran: Membuat Dataset untuk Machine Learning

## Pendahuluan

Dalam pengembangan model Machine Learning (ML), kualitas dataset memainkan peran krusial dalam menentukan kinerja model. Proses pembuatan dataset melibatkan beberapa langkah penting, mulai dari pengumpulan data hingga penyimpanan dalam format yang efisien untuk keperluan pelatihan model.

## 1. Membagi Dataset

Setelah mengumpulkan dan memberi label pada gambar, langkah pertama adalah membagi dataset menjadi tiga bagian utama:

- **Dataset Pelatihan (Training Set):** Digunakan untuk melatih model dengan proporsi sekitar 80% dari keseluruhan data.
- **Dataset Validasi (Validation Set):** Digunakan untuk mengatur hyperparameter model dengan proporsi sekitar 10%.
- **Dataset Pengujian (Test Set):** Digunakan untuk evaluasi akhir model, juga sekitar 10% dari data.

### Contoh Pembagian Dataset

```python
import numpy as np
from sklearn.model_selection import train_test_split

# Contoh data dummy
data = np.arange(1000)
labels = np.random.randint(0, 2, size=(1000,))

# Membagi data menjadi training (80%) dan sisa (20%)
X_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.2, random_state=42)

# Membagi sisa data menjadi validation (10%) dan test (10%)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
```

## 2. Format Penyimpanan Dataset

Untuk skala besar, penggunaan format CSV tidak efisien. Sebagai alternatif, **TensorFlow Records (TFRecords)** digunakan karena lebih cepat dalam membaca data.

### Membuat TFRecord

```python
import tensorflow as tf

def _bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def create_tfrecord(image_path, label, output_file):
    with tf.io.TFRecordWriter(output_file) as writer:
        image = tf.io.read_file(image_path)
        feature = {
            'image': _bytes_feature(image.numpy()),
            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))
        }
        example = tf.train.Example(features=tf.train.Features(feature=feature))
        writer.write(example.SerializeToString())

# Contoh penggunaan
create_tfrecord('path/to/image.jpg', 1, 'output.tfrecord')
```

## 3. Membaca Data dari TFRecord

```python
def _parse_function(proto):
    feature_description = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.int64)
    }
    parsed_example = tf.io.parse_single_example(proto, feature_description)
    image = tf.image.decode_jpeg(parsed_example['image'], channels=3)
    return image, parsed_example['label']

# Membaca dataset
raw_dataset = tf.data.TFRecordDataset(['output.tfrecord'])
parsed_dataset = raw_dataset.map(_parse_function)
```

## 4. Skala Produksi dengan Apache Beam

Untuk pemrosesan data dalam skala besar, **Apache Beam** dapat digunakan:

```python
import apache_beam as beam

def process_record(record):
    # Proses data di sini
    return record

with beam.Pipeline() as pipeline:
    (pipeline
     | 'Read Data' >> beam.io.ReadFromText('data.csv')
     | 'Process Data' >> beam.Map(process_record)
     | 'Write Data' >> beam.io.WriteToText('processed_data.csv')
    )
```

## Referensi

- Lakshmanan, V., GÃ¶rner, M., et al. *Practical Machine Learning for Computer Vision: End-to-End Machine Learning for Images*. O'Reilly Media.
- [GitHub Repository: Practical ML for Vision](https://github.com/GoogleCloudPlatform/practical-ml-vision-book)

---
Modul ini bertujuan memberikan pemahaman komprehensif mengenai proses pembuatan dataset, baik untuk skala kecil maupun besar. Dengan menguasai teknik ini, Anda dapat meningkatkan efisiensi dan performa model ML yang dikembangkan.


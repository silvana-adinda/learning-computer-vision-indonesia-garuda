# Beyond Convolution: Arsitektur Transformer

## Pendahuluan

Arsitektur tradisional untuk visi komputer banyak bergantung pada *convolutional neural networks* (CNN) untuk mengekstraksi fitur dari gambar. Namun, seiring berkembangnya kebutuhan akan model yang lebih fleksibel dan mampu menangani data dalam skala besar, muncul arsitektur baru yang melampaui batasan konvolusi, yaitu **Transformer**.

Arsitektur Transformer pertama kali diperkenalkan oleh Ashish Vaswani et al. dalam makalah berjudul *"Attention Is All You Need"* (2017)【16:0†https://arxiv.org/abs/1706.03762】. Konsep inti dari Transformer adalah **mekanisme perhatian (attention)**, yang memungkinkan model untuk fokus pada bagian penting dari data input.

## Konsep Dasar Transformer

Transformer terdiri dari dua komponen utama:

1. **Encoder:** Menerima input dan menghasilkan representasi yang diperkaya.
2. **Decoder:** Menggunakan representasi ini untuk menghasilkan output.

Kunci dari Transformer adalah **self-attention**, yang memungkinkan model untuk mempertimbangkan hubungan antar elemen dalam satu set data. Dalam konteks NLP, ini berarti memahami hubungan antar kata dalam sebuah kalimat. Sedangkan untuk visi komputer, konsep ini diadaptasi melalui **Vision Transformer (ViT)**.

## Vision Transformer (ViT)

Vision Transformer diperkenalkan oleh Dosovitskiy et al. dalam makalah *"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"* (2020)【16:0†https://arxiv.org/abs/2010.11929】. Ide utamanya adalah mengubah gambar menjadi serangkaian *patch* kecil, mirip dengan token dalam NLP.

### Langkah-Langkah Utama:

1. **Membagi Gambar Menjadi Patch:**
   ```python
   import tensorflow as tf

   patches = tf.image.extract_patches(
       images=images,
       sizes=[1, patch_size, patch_size, 1],
       strides=[1, patch_size, patch_size, 1],
       rates=[1, 1, 1, 1],
       padding="VALID",
   )
   ```

2. **Memberi Positional Encoding:**
   ```python
   encoded = (tf.keras.layers.Dense(64)(patches) +
              tf.keras.layers.Embedding(input_dim=100, output_dim=64)(positions))
   ```

3. **Menerapkan Transformer Encoder:**
   ```python
   from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add

   x = LayerNormalization()(encoded)
   attention_output = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)
   x = Add()([attention_output, encoded])
   x = LayerNormalization()(x)
   ```

## Studi Kasus: Klasifikasi Dataset Bunga

Dalam studi yang dilakukan oleh Google Cloud【16:1†https://github.com/GoogleCloudPlatform/practical-ml-vision-book】, Vision Transformer diterapkan untuk klasifikasi dataset *flowers104*. Model ini memerlukan data dalam jumlah besar untuk pretraining dan hasilnya menunjukkan akurasi sekitar 34% tanpa pretraining. Namun, dengan fine-tuning pada dataset yang lebih kecil, performanya meningkat signifikan.

## Kesimpulan

Arsitektur Transformer telah merevolusi berbagai bidang, dari NLP hingga visi komputer. Dengan fleksibilitas dan kemampuan menangani hubungan data yang kompleks, Transformer membuka peluang baru untuk pengembangan model AI yang lebih canggih.

---

**Referensi:**
- Vaswani, A., et al. (2017). *Attention Is All You Need*. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
- Dosovitskiy, A., et al. (2020). *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*. [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)
- [Practical Machine Learning for Computer Vision](https://github.com/GoogleCloudPlatform/practical-ml-vision-book)


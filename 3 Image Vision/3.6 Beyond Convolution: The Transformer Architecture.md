# Modul Pembelajaran: Beyond Convolution - Arsitektur Transformer

## Pendahuluan

Dalam dunia computer vision, arsitektur tradisional seperti Convolutional Neural Networks (CNN) telah mendominasi selama bertahun-tahun. Namun, seiring dengan meningkatnya kebutuhan terhadap model yang lebih fleksibel dan mampu menangani data dalam skala besar, muncul arsitektur baru yang disebut **Transformer**. Awalnya dikembangkan untuk pemrosesan bahasa alami (NLP) oleh Ashish Vaswani et al. dalam paper berjudul *"Attention Is All You Need"* (2017), arsitektur ini kini diadaptasi untuk computer vision melalui model yang dikenal sebagai **Vision Transformer (ViT)**.

## Konsep Dasar Transformer

Arsitektur Transformer berfokus pada mekanisme **attention**, yang memungkinkan model untuk menyoroti bagian input yang relevan saat membuat prediksi. Dalam NLP, ini berarti model dapat "memperhatikan" kata-kata tertentu dalam sebuah kalimat untuk memahami konteksnya. Misalnya, saat menerjemahkan kalimat Prancis *"ma chemise rouge"* menjadi *"my red shirt"*, model akan fokus pada kata *"rouge"* saat menerjemahkan menjadi *"red"*.

Mekanisme utama dalam Transformer meliputi:

1. **Self-Attention**: Memungkinkan model untuk mempertimbangkan hubungan antar elemen input.
2. **Positional Encoding**: Menambahkan informasi urutan ke dalam representasi data karena Transformer tidak memiliki struktur sekuensial bawaan seperti RNN.

## Vision Transformer (ViT)

Vision Transformer mengadaptasi konsep Transformer untuk gambar. Alih-alih memproses piksel secara langsung, gambar dipecah menjadi **patch** kecil (misalnya 16x16 piksel), yang diperlakukan seperti "kata" dalam NLP. Setiap patch ini kemudian diubah menjadi vektor dan diproses dengan cara yang mirip dengan teks.

### Langkah-langkah Utama:

1. **Ekstraksi Patch**:
```python
import tensorflow as tf

patches = tf.image.extract_patches(
    images=images,
    sizes=[1, 16, 16, 1],  # Ukuran patch 16x16
    strides=[1, 16, 16, 1],
    rates=[1, 1, 1, 1],
    padding="VALID"
)
```

2. **Encoding Patch**:
```python
encoded_patches = (tf.keras.layers.Dense(128)(patches) +  # Proyeksi linier
                    tf.keras.layers.Embedding(input_dim=patches.shape[1], output_dim=128)(tf.range(patches.shape[1])))
```

3. **Proses Attention**:
```python
x = tf.keras.layers.LayerNormalization()(encoded_patches)
attention_output = tf.keras.layers.MultiHeadAttention(
    num_heads=4, key_dim=64, dropout=0.1
)(x, x)

# Skip Connection
x = tf.keras.layers.Add()([attention_output, encoded_patches])
```

4. **Feedforward Layer (MLP)**:
```python
def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = tf.keras.layers.Dense(units, activation='relu')(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
    return x

x = mlp(x, hidden_units=[256, 128], dropout_rate=0.1)
```

## Studi Kasus Tambahan: Penerapan pada CIFAR-10

CIFAR-10 adalah dataset populer yang berisi 60.000 gambar berwarna dalam 10 kelas berbeda. Vision Transformer dapat digunakan untuk mengklasifikasikan gambar-gambar ini dengan melakukan pretraining di dataset besar seperti ImageNet, kemudian fine-tuning di CIFAR-10.

```python
# Fine-tuning Vision Transformer pada CIFAR-10
from tensorflow.keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
# Normalisasi data
x_train, x_test = x_train / 255.0, x_test / 255.0

# Model
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(32, 32, 3)),
    tf.keras.layers.Rescaling(1./255),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))
```

## Kesimpulan

Arsitektur Transformer, khususnya Vision Transformer, menunjukkan potensi luar biasa dalam computer vision. Dengan memanfaatkan mekanisme attention dan kemampuan untuk memproses data dalam skala besar, ViT menjadi alternatif yang kuat dibandingkan CNN tradisional, terutama untuk dataset besar.

## Referensi

- Vaswani, A., et al. (2017). *Attention Is All You Need*. 
- Dosovitskiy, A., et al. (2020). *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*.


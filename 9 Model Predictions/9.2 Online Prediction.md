# Online Prediction dengan TensorFlow Serving

Bagian ini menjelaskan implementasi prediksi online menggunakan TensorFlow Serving, modifikasi fungsi serving, dan penanganan data gambar dalam format bytes.

---

## TensorFlow Serving [1]

### Konsep Utama  
TensorFlow Serving adalah framework untuk men-deploy model ML ke lingkungan produksi. Memungkinkan:
- **Versioning model** (rollback/update mudah)
- **API REST/gRPC** untuk integrasi
- **Optimasi inferensi** (e.g., batch processing)

### Arsitektur  
![TF Serving Architecture](https://www.tensorflow.org/tfx/serving/images/serving_architecture.svg)  
*Gambar 1: Arsitektur TensorFlow Serving (Sumber: TensorFlow Docs)*

### Implementasi Dasar  
1. Simpan model dengan format SavedModel:
```python
model.save('model_dir/1/', save_format='tf')  # Versi 1
```
2. Jalankan TF Serving dalam Docker:
```bash
docker run -p 8501:8501 --name=tf_serving \
  -v "$(pwd)/model_dir:/models" -e MODEL_NAME=model \
  -t tensorflow/serving
```
3. Prediksi via REST API:
```python
import requests
response = requests.post(
    'http://localhost:8501/v1/models/model:predict',
    json={'instances': [input_data.tolist()]}
)
```

## Modifikasi Serving Function

### Tujuan  
Menyesuaikan logika preprocessing/postprocessing di sisi server [2].

**Contoh: Menambahkan Preprocessing Resize Gambar**
```python
# Definisikan fungsi custom dengan @tf.function
@tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])
def serve(image_bytes):
    # Decode bytes ke tensor gambar
    image = tf.io.decode_image(image_bytes[0], channels=3)
    # Resize dan normalisasi
    image = tf.image.resize(image, [224, 224]) / 255.0
    # Prediksi
    predictions = model(tf.expand_dims(image, axis=0))
    return {'probabilities': predictions.numpy().tolist()}

# Simpan model dengan signature custom
tf.saved_model.save(
    model,
    'model_dir/2/',
    signatures={'serving_default': serve}
)
```

## Penanganan Image Bytes

### 1. Menerima Gambar sebagai Base64  
**Client-Side (JavaScript):**
```javascript
const base64Data = canvas.toDataURL('image/jpeg').split(',')[1];
fetch('http://api/predict', {
  method: 'POST',
  body: JSON.stringify({ image: base64Data })
});
```
**Server-Side (Python):**
```python
import base64

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    image_bytes = base64.b64decode(data['image'])
    image = tf.io.decode_image(image_bytes)
    # ... proses lanjutan
```

### 2. Menerima Raw Bytes via gRPC [3]
```python
from tensorflow_serving.apis import prediction_service_pb2_grpc
from tensorflow_serving.apis.predict_pb2 import PredictRequest

channel = grpc.insecure_channel('localhost:8500')
stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)

request = PredictRequest()
request.model_spec.name = 'model'
request.inputs['image_bytes'].CopyFrom(
    tf.make_tensor_proto([image_bytes], shape=[1])
)
response = stub.Predict(request)
```

## Best Practices

**Batching:** Aktifkan batching untuk optimasi throughput [4]:
```bash
docker run ... --env TF_ENABLE_BATCHING=1 \
  --env TF_BATCH_TIMEOUT_MICROS=10000
```

**Monitoring:** Gunakan Prometheus + Grafana untuk melacak:
- Latency  
- Throughput  
- Error rate  

---

## Referensi
1. Olston, C., et al. (2017). TensorFlow-Serving: Flexible, High-Performance ML Serving. arXiv.
2. Google. (2022). Custom Serving Functions in TFX. TensorFlow Documentation.
3. Metz, C. (2019). gRPC Best Practices for Production. gRPC Blog.
4. Cheng, Y., et al. (2021). Optimizing ML Inference Performance. Google Research.

---



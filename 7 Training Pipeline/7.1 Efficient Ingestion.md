# Modul Pembelajaran: Efficient Ingestion

## Pendahuluan

Efficient ingestion adalah proses penting dalam pipeline pelatihan machine learning (ML) yang bertujuan untuk mengoptimalkan pengambilan (ingestion) data pelatihan dan validasi ke dalam model. Optimalisasi ini mencakup cara menyimpan data secara efisien, membaca data secara paralel, serta memaksimalkan penggunaan GPU. Dengan meningkatkan efisiensi ingestion, proses pelatihan menjadi lebih cepat dan hemat sumber daya.

## Strategi Efficient Ingestion

### 1. Penyimpanan Data yang Efisien

Menyimpan gambar sebagai file JPEG individual kurang efisien untuk keperluan ML. Alternatif yang lebih baik adalah menggunakan **TensorFlow Records (TFRecords)**. TFRecords memungkinkan penyimpanan data dalam format biner yang dapat dibaca secara efisien oleh TensorFlow. Beberapa keuntungan TFRecords:

- Membaca data dalam batch menggunakan satu koneksi jaringan.
- File berukuran ideal antara 10-100 MB untuk mengoptimalkan throughput.
- Format biner yang meminimalkan kebutuhan parsing saat membaca data.

**Contoh Kode:**
```python
import tensorflow as tf

def create_tfrecord(filename, label):
    img = tf.io.read_file(filename)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)
    img = tf.reshape(img, [-1])  # Flatten ke array 1D
    feature = {
        'image': tf.train.Feature(float_list=tf.train.FloatList(value=img.numpy())),
        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))
    }
    example = tf.train.Example(features=tf.train.Features(feature=feature))
    return example.SerializeToString()
```

### 2. Membaca Data Secara Paralel

Membaca data secara paralel dapat mengurangi bottleneck dari kecepatan perangkat penyimpanan. Dengan memanfaatkan beberapa CPU, kita dapat meningkatkan efisiensi.

**Contoh Kode:**
```python
ds = tf.data.TFRecordDataset(['data.tfrecord'])
ds = ds.map(lambda x: parse_function(x), num_parallel_calls=tf.data.AUTOTUNE)
```

### 3. Persiapan Data Secara Paralel dengan Pelatihan

Jika memungkinkan, preprocessing gambar dilakukan di CPU secara paralel saat GPU digunakan untuk pelatihan. Ini memastikan GPU tetap sibuk tanpa harus menunggu data diproses.

**Prefetching:**
```python
ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)
```

### 4. Memaksimalkan Utilisasi GPU

GPU jauh lebih cepat dibandingkan CPU untuk operasi matematis. Oleh karena itu, pastikan operasi seperti normalisasi atau augmentasi dilakukan di GPU.

**Contoh Kode dengan Vectorization:**
```python
ds = ds.batch(32).map(lambda x, y: augment_images(x, y), num_parallel_calls=tf.data.AUTOTUNE)
```

## Studi Kasus: Pengaruh Parallelization

Hasil pengukuran menunjukkan bahwa paralelisasi meningkatkan efisiensi waktu secara signifikan:

| Metode                    | CPU Time | Wall Time |
|---------------------------|----------|-----------|
| Plain                     | 7.53 s   | 7.99 s    |
| Parallel Map              | 8.30 s   | 5.94 s    |
| Interleave                | 8.60 s   | 5.47 s    |
| Interleave + Parallel Map | 8.44 s   | 5.23 s    |

(Sumber: Practical Machine Learning for Computer Vision)

## Kesimpulan

Efficient ingestion bukan hanya tentang membaca data lebih cepat, tetapi juga memastikan proses pelatihan berjalan lancar tanpa hambatan I/O. Dengan menggabungkan teknik penyimpanan efisien, paralelisasi, dan optimasi GPU, kita dapat mengurangi waktu pelatihan secara signifikan.

## Referensi
- Lakshmanan, V., GÃ¶rner, M., & Sarkar, R. (2020). *Practical Machine Learning for Computer Vision*. O'Reilly Media.
- [GitHub Repository Practical ML Vision](https://github.com/GoogleCloudPlatform/practical-ml-vision-book)

